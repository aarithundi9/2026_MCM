{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feaf09aa",
   "metadata": {},
   "source": [
    "# üîÄ Objective 2: Voting Method Comparison - Core Analysis\n",
    "## MCM Problem C 2026\n",
    "\n",
    "**Goal:** Apply BOTH voting methods (rank and percent) to ALL 34 seasons and create a \"counterfactual history\" of DWTS.\n",
    "\n",
    "### Key Questions:\n",
    "1. How often do the two methods produce different elimination outcomes?\n",
    "2. Which method favors fan votes more?\n",
    "3. What patterns emerge in method disagreement?\n",
    "\n",
    "### Table of Contents\n",
    "1. [Setup & Data Loading](#1-setup)\n",
    "2. [Method Simulation Functions](#2-methods)\n",
    "3. [Counterfactual History Generation](#3-counterfactual)\n",
    "4. [Basic Divergence Statistics](#4-divergence)\n",
    "5. [Uncertainty-Aware Analysis (Monte Carlo)](#5-uncertainty)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dc262e",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading <a id='1-setup'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5841377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy.stats import rankdata, spearmanr\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 60)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"‚úì Libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3402c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "DATA_PATH = Path('../../data')\n",
    "\n",
    "# Original contestant data\n",
    "df = pd.read_csv(DATA_PATH / '2026_MCM_Problem_C_Data.csv', na_values=['N/A', 'n/a', ''])\n",
    "\n",
    "# Fan vote estimates from Objective 1\n",
    "fan_votes_df = pd.read_csv(DATA_PATH / 'obj1' / 'fan_vote_estimates.csv')\n",
    "\n",
    "print(f\"Original data: {df.shape[0]} contestants\")\n",
    "print(f\"Fan vote estimates: {fan_votes_df.shape[0]} records\")\n",
    "print(f\"\\nFan votes columns: {list(fan_votes_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e61d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for uncertainty bounds from Objective 1\n",
    "has_uncertainty = 'fan_votes_min' in fan_votes_df.columns and 'fan_votes_max' in fan_votes_df.columns\n",
    "\n",
    "if has_uncertainty:\n",
    "    print(\"‚úì Uncertainty bounds available\")\n",
    "    print(f\"  - fan_votes_min: {fan_votes_df['fan_votes_min'].notna().sum()} values\")\n",
    "    print(f\"  - fan_votes_max: {fan_votes_df['fan_votes_max'].notna().sum()} values\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No uncertainty bounds - will use point estimates only\")\n",
    "\n",
    "# Overview\n",
    "print(f\"\\nSeasons: {fan_votes_df['season'].min()} to {fan_votes_df['season'].max()}\")\n",
    "print(f\"Unique weeks with eliminations: {fan_votes_df[fan_votes_df['was_eliminated']].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be16a1a6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Method Simulation Functions <a id='2-methods'></a>\n",
    "\n",
    "### Mathematical Definitions\n",
    "\n",
    "**Rank-Based Method (S1-2, S28-34):**\n",
    "$$S_i = R_i^{judge} + R_i^{fan}$$\n",
    "Eliminated: $\\arg\\max_i S_i$ (highest combined rank = worst)\n",
    "\n",
    "**Percent-Based Method (S3-27):**\n",
    "$$S_i = P_i^{judge} + P_i^{fan} = \\frac{J_i}{\\sum J} + \\frac{F_i}{\\sum F}$$\n",
    "Eliminated: $\\arg\\min_i S_i$ (lowest combined percentage = worst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd22db0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_rank_elimination(judge_scores, fan_votes, tie_method='average'):\n",
    "    \"\"\"\n",
    "    Simulate elimination under RANK-BASED method.\n",
    "    \n",
    "    Args:\n",
    "        judge_scores: Array of judge scores (higher = better)\n",
    "        fan_votes: Array of fan votes (higher = better)\n",
    "        tie_method: How to handle ties in ranking\n",
    "        \n",
    "    Returns:\n",
    "        dict with elimination index, combined ranks, and metadata\n",
    "    \"\"\"\n",
    "    n = len(judge_scores)\n",
    "    \n",
    "    # Rank in descending order (1 = highest score/most votes)\n",
    "    judge_ranks = rankdata(-np.array(judge_scores), method=tie_method)\n",
    "    fan_ranks = rankdata(-np.array(fan_votes), method=tie_method)\n",
    "    \n",
    "    # Combined rank sum (higher = worse)\n",
    "    combined_ranks = judge_ranks + fan_ranks\n",
    "    \n",
    "    # Eliminated = highest combined rank\n",
    "    eliminated_idx = np.argmax(combined_ranks)\n",
    "    \n",
    "    # Margin: how much higher than 2nd-worst?\n",
    "    sorted_ranks = np.sort(combined_ranks)\n",
    "    margin = combined_ranks[eliminated_idx] - sorted_ranks[-2] if n > 1 else 0\n",
    "    \n",
    "    return {\n",
    "        'eliminated_idx': eliminated_idx,\n",
    "        'judge_ranks': judge_ranks,\n",
    "        'fan_ranks': fan_ranks,\n",
    "        'combined_ranks': combined_ranks,\n",
    "        'margin': margin\n",
    "    }\n",
    "\n",
    "\n",
    "def simulate_percent_elimination(judge_scores, fan_votes):\n",
    "    \"\"\"\n",
    "    Simulate elimination under PERCENT-BASED method.\n",
    "    \n",
    "    Args:\n",
    "        judge_scores: Array of judge scores\n",
    "        fan_votes: Array of fan votes\n",
    "        \n",
    "    Returns:\n",
    "        dict with elimination index, combined percentages, and metadata\n",
    "    \"\"\"\n",
    "    judge_scores = np.array(judge_scores)\n",
    "    fan_votes = np.array(fan_votes)\n",
    "    \n",
    "    # Convert to percentages\n",
    "    judge_pct = judge_scores / judge_scores.sum()\n",
    "    fan_pct = fan_votes / fan_votes.sum()\n",
    "    \n",
    "    # Combined percentage (higher = better)\n",
    "    combined_pct = judge_pct + fan_pct\n",
    "    \n",
    "    # Eliminated = lowest combined percentage\n",
    "    eliminated_idx = np.argmin(combined_pct)\n",
    "    \n",
    "    # Margin: how much lower than 2nd-lowest?\n",
    "    sorted_pct = np.sort(combined_pct)\n",
    "    margin = sorted_pct[1] - combined_pct[eliminated_idx] if len(judge_scores) > 1 else 0\n",
    "    \n",
    "    return {\n",
    "        'eliminated_idx': eliminated_idx,\n",
    "        'judge_pct': judge_pct,\n",
    "        'fan_pct': fan_pct,\n",
    "        'combined_pct': combined_pct,\n",
    "        'margin': margin\n",
    "    }\n",
    "\n",
    "\n",
    "def simulate_judges_bottom2(judge_scores, fan_votes, method='percent'):\n",
    "    \"\"\"\n",
    "    Simulate the S28+ rule: identify bottom 2, then judges choose.\n",
    "    \n",
    "    Bottom 2 determined by combined scores, then judges eliminate\n",
    "    the one with the lower judge score.\n",
    "    \"\"\"\n",
    "    judge_scores = np.array(judge_scores)\n",
    "    fan_votes = np.array(fan_votes)\n",
    "    \n",
    "    if method == 'percent':\n",
    "        result = simulate_percent_elimination(judge_scores, fan_votes)\n",
    "        combined = result['combined_pct']\n",
    "        # Bottom 2 = two lowest combined percentages\n",
    "        bottom2_idx = np.argsort(combined)[:2]\n",
    "    else:  # rank\n",
    "        result = simulate_rank_elimination(judge_scores, fan_votes)\n",
    "        combined = result['combined_ranks']\n",
    "        # Bottom 2 = two highest combined ranks\n",
    "        bottom2_idx = np.argsort(combined)[-2:]\n",
    "    \n",
    "    # Judges choose: eliminate the one with lower judge score\n",
    "    if judge_scores[bottom2_idx[0]] <= judge_scores[bottom2_idx[1]]:\n",
    "        eliminated_idx = bottom2_idx[0]\n",
    "    else:\n",
    "        eliminated_idx = bottom2_idx[1]\n",
    "    \n",
    "    return {\n",
    "        'eliminated_idx': eliminated_idx,\n",
    "        'bottom2_idx': bottom2_idx,\n",
    "        'combined': combined\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úì Simulation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f5e3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the functions with example data\n",
    "example_judge = [28, 25, 30, 22]\n",
    "example_fan = [2.5e6, 3.2e6, 1.8e6, 2.0e6]\n",
    "names = ['Alice', 'Bob', 'Carol', 'Dave']\n",
    "\n",
    "rank_result = simulate_rank_elimination(example_judge, example_fan)\n",
    "pct_result = simulate_percent_elimination(example_judge, example_fan)\n",
    "b2_result = simulate_judges_bottom2(example_judge, example_fan, method='percent')\n",
    "\n",
    "print(\"Example with 4 contestants:\")\n",
    "print(f\"  Judge scores: {example_judge}\")\n",
    "print(f\"  Fan votes (M): {[v/1e6 for v in example_fan]}\")\n",
    "print()\n",
    "print(f\"RANK method ‚Üí Eliminates: {names[rank_result['eliminated_idx']]}\")\n",
    "print(f\"  Combined ranks: {rank_result['combined_ranks']}\")\n",
    "print()\n",
    "print(f\"PERCENT method ‚Üí Eliminates: {names[pct_result['eliminated_idx']]}\")\n",
    "print(f\"  Combined %: {[f'{p:.1%}' for p in pct_result['combined_pct']]}\")\n",
    "print()\n",
    "print(f\"JUDGES BOTTOM 2 ‚Üí Eliminates: {names[b2_result['eliminated_idx']]}\")\n",
    "print(f\"  Bottom 2: {[names[i] for i in b2_result['bottom2_idx']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37cd27f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Counterfactual History Generation <a id='3-counterfactual'></a>\n",
    "\n",
    "Apply BOTH methods to ALL seasons, creating a complete counterfactual history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9009e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_counterfactual_history(fan_votes_df):\n",
    "    \"\"\"\n",
    "    Generate counterfactual elimination history under all three methods.\n",
    "    \n",
    "    Returns DataFrame with actual vs counterfactual eliminations.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Group by season and week\n",
    "    grouped = fan_votes_df.groupby(['season', 'week'])\n",
    "    \n",
    "    for (season, week), week_df in tqdm(grouped, desc=\"Processing weeks\"):\n",
    "        # Skip weeks with no elimination\n",
    "        if week_df['was_eliminated'].sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        # Get data\n",
    "        judge_scores = week_df['judge_score'].values\n",
    "        fan_votes = week_df['fan_votes_estimate'].values\n",
    "        contestants = week_df['celebrity_name'].values\n",
    "        actual_method = week_df['method'].iloc[0]\n",
    "        \n",
    "        # Find actual eliminated contestant\n",
    "        actual_elim_mask = week_df['was_eliminated'].values\n",
    "        actual_elim_idx = np.where(actual_elim_mask)[0][0]\n",
    "        actual_elim_name = contestants[actual_elim_idx]\n",
    "        \n",
    "        # Simulate all three methods\n",
    "        rank_result = simulate_rank_elimination(judge_scores, fan_votes)\n",
    "        pct_result = simulate_percent_elimination(judge_scores, fan_votes)\n",
    "        b2_rank_result = simulate_judges_bottom2(judge_scores, fan_votes, method='rank')\n",
    "        b2_pct_result = simulate_judges_bottom2(judge_scores, fan_votes, method='percent')\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'season': season,\n",
    "            'week': week,\n",
    "            'n_contestants': len(contestants),\n",
    "            'actual_method': actual_method,\n",
    "            'actual_eliminated': actual_elim_name,\n",
    "            'actual_elim_idx': actual_elim_idx,\n",
    "            \n",
    "            # Counterfactual eliminations\n",
    "            'rank_would_eliminate': contestants[rank_result['eliminated_idx']],\n",
    "            'pct_would_eliminate': contestants[pct_result['eliminated_idx']],\n",
    "            'b2_rank_would_eliminate': contestants[b2_rank_result['eliminated_idx']],\n",
    "            'b2_pct_would_eliminate': contestants[b2_pct_result['eliminated_idx']],\n",
    "            \n",
    "            # Agreement flags\n",
    "            'rank_matches_actual': contestants[rank_result['eliminated_idx']] == actual_elim_name,\n",
    "            'pct_matches_actual': contestants[pct_result['eliminated_idx']] == actual_elim_name,\n",
    "            'methods_agree': rank_result['eliminated_idx'] == pct_result['eliminated_idx'],\n",
    "            \n",
    "            # Margins (how clearcut was the elimination?)\n",
    "            'rank_margin': rank_result['margin'],\n",
    "            'pct_margin': pct_result['margin'],\n",
    "            \n",
    "            # Judge-Fan alignment\n",
    "            'jfac': spearmanr(judge_scores, fan_votes)[0] if len(judge_scores) > 2 else np.nan\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"‚úì Counterfactual generator defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9436c6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate counterfactual history\n",
    "print(\"Generating counterfactual history for all seasons...\")\n",
    "counterfactual_df = generate_counterfactual_history(fan_votes_df)\n",
    "\n",
    "print(f\"\\n‚úì Generated {len(counterfactual_df)} elimination records\")\n",
    "print(f\"  Seasons: {counterfactual_df['season'].min()} to {counterfactual_df['season'].max()}\")\n",
    "\n",
    "counterfactual_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb4f076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save counterfactual history\n",
    "OUTPUT_PATH = DATA_PATH / 'obj2'\n",
    "OUTPUT_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "counterfactual_df.to_csv(OUTPUT_PATH / 'counterfactual_history.csv', index=False)\n",
    "print(f\"‚úì Saved to {OUTPUT_PATH / 'counterfactual_history.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9504ab8f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Basic Divergence Statistics <a id='4-divergence'></a>\n",
    "\n",
    "How often do the methods disagree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93caa981",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"OUTCOME DIVERGENCE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Overall statistics\n",
    "total_weeks = len(counterfactual_df)\n",
    "methods_agree = counterfactual_df['methods_agree'].sum()\n",
    "methods_disagree = total_weeks - methods_agree\n",
    "\n",
    "print(f\"\\nüìä Overall Results (Point Estimates):\")\n",
    "print(f\"   Total elimination weeks: {total_weeks}\")\n",
    "print(f\"   Methods AGREE: {methods_agree} ({methods_agree/total_weeks:.1%})\")\n",
    "print(f\"   Methods DISAGREE: {methods_disagree} ({methods_disagree/total_weeks:.1%})\")\n",
    "\n",
    "# By actual method used\n",
    "print(f\"\\nüìà Disagreement by Actual Method Used:\")\n",
    "for method, group in counterfactual_df.groupby('actual_method'):\n",
    "    n = len(group)\n",
    "    disagree = (~group['methods_agree']).sum()\n",
    "    print(f\"   {method.upper()}: {disagree}/{n} = {disagree/n:.1%} disagreement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d8f9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all cases where methods disagree\n",
    "disagreements = counterfactual_df[~counterfactual_df['methods_agree']].copy()\n",
    "disagreements = disagreements.sort_values(['season', 'week'])\n",
    "\n",
    "print(f\"\\nüî¥ ALL {len(disagreements)} DISAGREEMENT CASES:\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "display_cols = ['season', 'week', 'actual_eliminated', 'rank_would_eliminate', \n",
    "                'pct_would_eliminate', 'actual_method', 'jfac']\n",
    "\n",
    "if len(disagreements) > 0:\n",
    "    print(disagreements[display_cols].to_string(index=False))\n",
    "else:\n",
    "    print(\"No disagreements found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e65e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize disagreement patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Disagreement by season\n",
    "ax1 = axes[0, 0]\n",
    "season_disagree = counterfactual_df.groupby('season').agg({\n",
    "    'methods_agree': lambda x: (~x).sum(),\n",
    "    'week': 'count'\n",
    "}).reset_index()\n",
    "season_disagree.columns = ['season', 'disagreements', 'total_weeks']\n",
    "season_disagree['disagree_pct'] = season_disagree['disagreements'] / season_disagree['total_weeks'] * 100\n",
    "\n",
    "# Color by actual method\n",
    "colors = ['steelblue' if s in [1, 2] or s >= 28 else 'coral' \n",
    "          for s in season_disagree['season']]\n",
    "\n",
    "ax1.bar(season_disagree['season'], season_disagree['disagree_pct'], color=colors, edgecolor='black')\n",
    "ax1.axhline(methods_disagree/total_weeks*100, color='red', linestyle='--', \n",
    "            label=f'Overall: {methods_disagree/total_weeks:.1%}')\n",
    "ax1.set_xlabel('Season')\n",
    "ax1.set_ylabel('Disagreement Rate (%)')\n",
    "ax1.set_title('Method Disagreement by Season\\n(Blue=Rank seasons, Orange=Percent seasons)')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Disagreement by week number\n",
    "ax2 = axes[0, 1]\n",
    "week_disagree = counterfactual_df.groupby('week').agg({\n",
    "    'methods_agree': lambda x: (~x).mean() * 100\n",
    "}).reset_index()\n",
    "week_disagree.columns = ['week', 'disagree_pct']\n",
    "\n",
    "ax2.bar(week_disagree['week'], week_disagree['disagree_pct'], color='purple', edgecolor='black')\n",
    "ax2.set_xlabel('Week')\n",
    "ax2.set_ylabel('Disagreement Rate (%)')\n",
    "ax2.set_title('Method Disagreement by Week Number')\n",
    "\n",
    "# Plot 3: Disagreement vs Judge-Fan Alignment (JFAC)\n",
    "ax3 = axes[1, 0]\n",
    "valid_jfac = counterfactual_df[counterfactual_df['jfac'].notna()].copy()\n",
    "agree_jfac = valid_jfac[valid_jfac['methods_agree']]['jfac']\n",
    "disagree_jfac = valid_jfac[~valid_jfac['methods_agree']]['jfac']\n",
    "\n",
    "ax3.hist(agree_jfac, bins=20, alpha=0.5, label=f'Agree (n={len(agree_jfac)})', color='green')\n",
    "ax3.hist(disagree_jfac, bins=20, alpha=0.5, label=f'Disagree (n={len(disagree_jfac)})', color='red')\n",
    "ax3.axvline(agree_jfac.mean(), color='green', linestyle='--', linewidth=2)\n",
    "ax3.axvline(disagree_jfac.mean(), color='red', linestyle='--', linewidth=2)\n",
    "ax3.set_xlabel('Judge-Fan Alignment (JFAC)')\n",
    "ax3.set_ylabel('Count')\n",
    "ax3.set_title('JFAC Distribution: Agreement vs Disagreement Cases')\n",
    "ax3.legend()\n",
    "\n",
    "# Plot 4: Disagreement by number of contestants\n",
    "ax4 = axes[1, 1]\n",
    "n_disagree = counterfactual_df.groupby('n_contestants').agg({\n",
    "    'methods_agree': lambda x: (~x).mean() * 100,\n",
    "    'week': 'count'\n",
    "}).reset_index()\n",
    "n_disagree.columns = ['n_contestants', 'disagree_pct', 'count']\n",
    "\n",
    "ax4.bar(n_disagree['n_contestants'], n_disagree['disagree_pct'], color='teal', edgecolor='black')\n",
    "ax4.set_xlabel('Number of Contestants')\n",
    "ax4.set_ylabel('Disagreement Rate (%)')\n",
    "ax4.set_title('Method Disagreement by Remaining Contestants')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'disagreement_patterns.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Saved to {OUTPUT_PATH / 'disagreement_patterns.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea81a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insight: When do methods disagree?\n",
    "print(\"=\"*70)\n",
    "print(\"KEY INSIGHT: When Do Methods Disagree?\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if len(disagree_jfac) > 0:\n",
    "    print(f\"\\nüìä Judge-Fan Alignment (JFAC):\")\n",
    "    print(f\"   When methods AGREE: Mean JFAC = {agree_jfac.mean():.3f}\")\n",
    "    print(f\"   When methods DISAGREE: Mean JFAC = {disagree_jfac.mean():.3f}\")\n",
    "    print(f\"   ‚Üí Disagreements occur when judges and fans are LESS aligned!\")\n",
    "\n",
    "# Margin analysis\n",
    "print(f\"\\nüìä Elimination Margins:\")\n",
    "agree_margin_rank = counterfactual_df[counterfactual_df['methods_agree']]['rank_margin'].mean()\n",
    "disagree_margin_rank = counterfactual_df[~counterfactual_df['methods_agree']]['rank_margin'].mean()\n",
    "print(f\"   Rank margin when AGREE: {agree_margin_rank:.2f}\")\n",
    "print(f\"   Rank margin when DISAGREE: {disagree_margin_rank:.2f}\")\n",
    "print(f\"   ‚Üí Disagreements occur when eliminations are CLOSER!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14509ab6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Uncertainty-Aware Analysis (Monte Carlo) <a id='5-uncertainty'></a>\n",
    "\n",
    "**Critical Question:** Our fan vote estimates have uncertainty. How robust are our counterfactual conclusions?\n",
    "\n",
    "We'll sample from the feasible fan vote region and check if disagreement conclusions hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28186f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_counterfactual(week_df, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Run Monte Carlo simulation over uncertainty bounds.\n",
    "    \n",
    "    For each sample, draw fan votes uniformly from [min, max] bounds\n",
    "    and check if methods agree/disagree.\n",
    "    \n",
    "    Returns probability of disagreement.\n",
    "    \"\"\"\n",
    "    judge_scores = week_df['judge_score'].values\n",
    "    fan_votes_est = week_df['fan_votes_estimate'].values\n",
    "    \n",
    "    # Check if uncertainty bounds exist\n",
    "    if 'fan_votes_min' in week_df.columns and week_df['fan_votes_min'].notna().all():\n",
    "        fan_min = week_df['fan_votes_min'].values\n",
    "        fan_max = week_df['fan_votes_max'].values\n",
    "    else:\n",
    "        # If no bounds, use point estimate ¬± 20% as proxy\n",
    "        fan_min = fan_votes_est * 0.8\n",
    "        fan_max = fan_votes_est * 1.2\n",
    "    \n",
    "    n = len(judge_scores)\n",
    "    \n",
    "    # Track outcomes\n",
    "    rank_elims = []\n",
    "    pct_elims = []\n",
    "    agree_count = 0\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Sample fan votes uniformly from bounds\n",
    "        fan_sample = np.random.uniform(fan_min, fan_max)\n",
    "        \n",
    "        # Normalize to ensure they sum to reasonable total\n",
    "        fan_sample = fan_sample / fan_sample.sum() * fan_votes_est.sum()\n",
    "        \n",
    "        # Simulate both methods\n",
    "        rank_result = simulate_rank_elimination(judge_scores, fan_sample)\n",
    "        pct_result = simulate_percent_elimination(judge_scores, fan_sample)\n",
    "        \n",
    "        rank_elims.append(rank_result['eliminated_idx'])\n",
    "        pct_elims.append(pct_result['eliminated_idx'])\n",
    "        \n",
    "        if rank_result['eliminated_idx'] == pct_result['eliminated_idx']:\n",
    "            agree_count += 1\n",
    "    \n",
    "    # Compute statistics\n",
    "    rank_elim_probs = np.bincount(rank_elims, minlength=n) / n_samples\n",
    "    pct_elim_probs = np.bincount(pct_elims, minlength=n) / n_samples\n",
    "    \n",
    "    return {\n",
    "        'p_agree': agree_count / n_samples,\n",
    "        'p_disagree': 1 - agree_count / n_samples,\n",
    "        'rank_elim_probs': rank_elim_probs,\n",
    "        'pct_elim_probs': pct_elim_probs,\n",
    "        'n_samples': n_samples\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úì Monte Carlo counterfactual function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1363ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Monte Carlo on all weeks\n",
    "print(\"Running Monte Carlo uncertainty analysis...\")\n",
    "print(\"(This may take a few minutes)\")\n",
    "\n",
    "mc_results = []\n",
    "\n",
    "grouped = fan_votes_df.groupby(['season', 'week'])\n",
    "\n",
    "for (season, week), week_df in tqdm(grouped, desc=\"Monte Carlo\"):\n",
    "    if week_df['was_eliminated'].sum() == 0:\n",
    "        continue\n",
    "    \n",
    "    mc = monte_carlo_counterfactual(week_df, n_samples=500)\n",
    "    \n",
    "    mc_results.append({\n",
    "        'season': season,\n",
    "        'week': week,\n",
    "        'p_agree': mc['p_agree'],\n",
    "        'p_disagree': mc['p_disagree']\n",
    "    })\n",
    "\n",
    "mc_df = pd.DataFrame(mc_results)\n",
    "print(f\"\\n‚úì Monte Carlo complete for {len(mc_df)} weeks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd6c50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with counterfactual results\n",
    "counterfactual_df = counterfactual_df.merge(mc_df, on=['season', 'week'], how='left')\n",
    "\n",
    "# Summary statistics\n",
    "print(\"=\"*70)\n",
    "print(\"UNCERTAINTY-AWARE DISAGREEMENT ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Point estimate said disagree - what does MC say?\n",
    "point_disagree = counterfactual_df[~counterfactual_df['methods_agree']]\n",
    "point_agree = counterfactual_df[counterfactual_df['methods_agree']]\n",
    "\n",
    "print(f\"\\nüìä Point Estimate: {len(point_disagree)} disagreements\")\n",
    "print(f\"\\n   Of those 'disagreements':\")\n",
    "print(f\"   - Always disagree (p_disagree > 0.95): {(point_disagree['p_disagree'] > 0.95).sum()}\")\n",
    "print(f\"   - Usually disagree (p_disagree > 0.75): {(point_disagree['p_disagree'] > 0.75).sum()}\")\n",
    "print(f\"   - Uncertain (0.25 < p < 0.75): {((point_disagree['p_disagree'] > 0.25) & (point_disagree['p_disagree'] < 0.75)).sum()}\")\n",
    "print(f\"   - Usually agree (p_disagree < 0.25): {(point_disagree['p_disagree'] < 0.25).sum()}\")\n",
    "\n",
    "print(f\"\\nüìä Mean P(disagree) across all weeks: {mc_df['p_disagree'].mean():.1%}\")\n",
    "print(f\"   95% CI: [{mc_df['p_disagree'].quantile(0.025):.1%}, {mc_df['p_disagree'].quantile(0.975):.1%}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8921c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize uncertainty in disagreement\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Distribution of P(disagree)\n",
    "ax1 = axes[0]\n",
    "ax1.hist(mc_df['p_disagree'], bins=30, color='purple', alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(0.5, color='red', linestyle='--', label='50% threshold')\n",
    "ax1.axvline(mc_df['p_disagree'].mean(), color='orange', linestyle='-', linewidth=2,\n",
    "            label=f'Mean: {mc_df[\"p_disagree\"].mean():.1%}')\n",
    "ax1.set_xlabel('P(Methods Disagree)')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Distribution of Disagreement Probability\\n(Monte Carlo, 500 samples per week)')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: P(disagree) by season\n",
    "ax2 = axes[1]\n",
    "season_mc = mc_df.groupby(counterfactual_df['season'])['p_disagree'].mean()\n",
    "colors = ['steelblue' if s in [1, 2] or s >= 28 else 'coral' for s in season_mc.index]\n",
    "ax2.bar(season_mc.index, season_mc.values * 100, color=colors, edgecolor='black')\n",
    "ax2.axhline(mc_df['p_disagree'].mean() * 100, color='red', linestyle='--',\n",
    "            label=f'Overall mean: {mc_df[\"p_disagree\"].mean():.1%}')\n",
    "ax2.set_xlabel('Season')\n",
    "ax2.set_ylabel('Mean P(Disagree) %')\n",
    "ax2.set_title('Uncertainty-Aware Disagreement Rate by Season')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'uncertainty_disagreement.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71a7048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save enhanced counterfactual data\n",
    "counterfactual_df.to_csv(OUTPUT_PATH / 'counterfactual_history_with_uncertainty.csv', index=False)\n",
    "print(f\"‚úì Saved enhanced results to {OUTPUT_PATH / 'counterfactual_history_with_uncertainty.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4325b8d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Findings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f435dee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"NOTEBOOK SUMMARY: Method Comparison Core Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "üìä OVERALL STATISTICS:\n",
    "   - Total elimination weeks analyzed: {len(counterfactual_df)}\n",
    "   - Point estimate disagreement rate: {(~counterfactual_df['methods_agree']).mean():.1%}\n",
    "   - Monte Carlo mean P(disagree): {mc_df['p_disagree'].mean():.1%}\n",
    "\n",
    "üîç KEY INSIGHTS:\n",
    "   1. Methods disagree more when Judge-Fan alignment (JFAC) is LOW\n",
    "   2. Disagreements tend to occur in CLOSE eliminations (small margins)\n",
    "   3. Uncertainty bounds matter: some \"disagreements\" become uncertain under MC\n",
    "\n",
    "üìÅ FILES CREATED:\n",
    "   - {OUTPUT_PATH / 'counterfactual_history.csv'}\n",
    "   - {OUTPUT_PATH / 'counterfactual_history_with_uncertainty.csv'}\n",
    "   - {OUTPUT_PATH / 'disagreement_patterns.png'}\n",
    "   - {OUTPUT_PATH / 'uncertainty_disagreement.png'}\n",
    "\n",
    "‚û°Ô∏è NEXT: See 06_divergence_analysis.ipynb for deeper divergence metrics\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f437945e",
   "metadata": {},
   "source": [
    "# üìä Objective 2: Divergence Analysis & Novel Metrics\n",
    "## MCM Problem C 2026\n",
    "\n",
    "**Goal:** Develop rigorous metrics to compare voting methods beyond simple agreement counts.\n",
    "\n",
    "### Novel Metrics Introduced:\n",
    "1. **Outcome Divergence Score (ODS)** - Seasonal measure of method disagreement\n",
    "2. **Judge-Fan Alignment Coefficient (JFAC)** - Spearman correlation between rankings\n",
    "3. **Margin of Safety (MoS)** - How close was the elimination?\n",
    "4. **Method Sensitivity Index (MSI)** - Minimum vote change to flip outcome\n",
    "5. **Underdog Survival Probability (USP)** - Does a method favor low-scorers?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de16d9ef",
   "metadata": {},
   "source": [
    "## 1. Setup & Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a3e592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy.stats import rankdata, spearmanr, mannwhitneyu\n",
    "from scipy.optimize import minimize\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "DATA_PATH = Path('../../data')\n",
    "OUTPUT_PATH = DATA_PATH / 'obj2'\n",
    "\n",
    "print(\"‚úì Libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b3e847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from previous notebook\n",
    "fan_votes_df = pd.read_csv(DATA_PATH / 'obj1' / 'fan_vote_estimates.csv')\n",
    "counterfactual_df = pd.read_csv(OUTPUT_PATH / 'counterfactual_history_with_uncertainty.csv')\n",
    "\n",
    "print(f\"Loaded {len(fan_votes_df)} fan vote estimates\")\n",
    "print(f\"Loaded {len(counterfactual_df)} counterfactual records\")\n",
    "\n",
    "counterfactual_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0df3d3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Outcome Divergence Score (ODS)\n",
    "\n",
    "**Definition:** The fraction of elimination weeks where rank and percent methods would produce different outcomes.\n",
    "\n",
    "$$\\text{ODS}_{season} = \\frac{1}{W} \\sum_{w=1}^{W} \\mathbb{1}[E^{rank}_w \\neq E^{pct}_w]$$\n",
    "\n",
    "where $W$ is the number of elimination weeks in the season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a9ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ODS by season\n",
    "ods_by_season = counterfactual_df.groupby('season').agg({\n",
    "    'methods_agree': lambda x: (~x).sum(),  # Number of disagreements\n",
    "    'week': 'count',  # Total weeks\n",
    "    'p_disagree': 'mean'  # Mean MC probability of disagreement\n",
    "}).reset_index()\n",
    "\n",
    "ods_by_season.columns = ['season', 'disagreements', 'total_weeks', 'mean_p_disagree']\n",
    "ods_by_season['ODS'] = ods_by_season['disagreements'] / ods_by_season['total_weeks']\n",
    "ods_by_season['ODS_mc'] = ods_by_season['mean_p_disagree']  # Uncertainty-aware ODS\n",
    "\n",
    "# Add actual method used\n",
    "ods_by_season['actual_method'] = ods_by_season['season'].apply(\n",
    "    lambda s: 'rank' if s in [1, 2] or s >= 28 else 'percent'\n",
    ")\n",
    "\n",
    "print(\"Outcome Divergence Score by Season:\")\n",
    "print(\"=\"*60)\n",
    "print(ods_by_season[['season', 'actual_method', 'disagreements', 'total_weeks', 'ODS', 'ODS_mc']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42304754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ODS\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: ODS by season (point estimate vs MC)\n",
    "ax1 = axes[0]\n",
    "x = ods_by_season['season']\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, ods_by_season['ODS'] * 100, width, \n",
    "                label='Point Estimate ODS', color='steelblue', alpha=0.8)\n",
    "bars2 = ax1.bar(x + width/2, ods_by_season['ODS_mc'] * 100, width,\n",
    "                label='Monte Carlo ODS', color='coral', alpha=0.8)\n",
    "\n",
    "ax1.axhline(ods_by_season['ODS'].mean() * 100, color='steelblue', linestyle='--', alpha=0.5)\n",
    "ax1.axhline(ods_by_season['ODS_mc'].mean() * 100, color='coral', linestyle='--', alpha=0.5)\n",
    "\n",
    "ax1.set_xlabel('Season')\n",
    "ax1.set_ylabel('Outcome Divergence Score (%)')\n",
    "ax1.set_title('ODS by Season: How Often Do Methods Disagree?')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: ODS comparison by actual method\n",
    "ax2 = axes[1]\n",
    "rank_seasons = ods_by_season[ods_by_season['actual_method'] == 'rank']\n",
    "pct_seasons = ods_by_season[ods_by_season['actual_method'] == 'percent']\n",
    "\n",
    "data_to_plot = [\n",
    "    rank_seasons['ODS'] * 100,\n",
    "    pct_seasons['ODS'] * 100\n",
    "]\n",
    "\n",
    "bp = ax2.boxplot(data_to_plot, labels=['Rank Seasons\\n(S1-2, S28-34)', 'Percent Seasons\\n(S3-27)'],\n",
    "                 patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('steelblue')\n",
    "bp['boxes'][1].set_facecolor('coral')\n",
    "\n",
    "ax2.set_ylabel('ODS (%)')\n",
    "ax2.set_title('ODS Distribution by Actual Method Used')\n",
    "\n",
    "# Add means\n",
    "means = [rank_seasons['ODS'].mean() * 100, pct_seasons['ODS'].mean() * 100]\n",
    "ax2.scatter([1, 2], means, color='red', s=100, zorder=5, marker='D', label='Mean')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'ods_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Overall ODS (Point Estimate): {ods_by_season['ODS'].mean():.1%}\")\n",
    "print(f\"üìä Overall ODS (Monte Carlo): {ods_by_season['ODS_mc'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59d0fe2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Judge-Fan Alignment Coefficient (JFAC)\n",
    "\n",
    "**Definition:** Spearman correlation between judge score ranking and fan vote ranking.\n",
    "\n",
    "$$\\text{JFAC}_w = \\rho_s(R^{judge}, R^{fan})$$\n",
    "\n",
    "- JFAC = 1: Judges and fans agree perfectly\n",
    "- JFAC = 0: No correlation\n",
    "- JFAC = -1: Complete disagreement\n",
    "\n",
    "**Hypothesis:** Low JFAC ‚Üí Methods more likely to disagree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5b1776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute JFAC for each week (already done in counterfactual_df)\n",
    "jfac_df = counterfactual_df[['season', 'week', 'jfac', 'methods_agree', 'p_disagree']].copy()\n",
    "jfac_df = jfac_df.dropna(subset=['jfac'])\n",
    "\n",
    "print(\"JFAC Statistics:\")\n",
    "print(\"=\"*50)\n",
    "print(jfac_df['jfac'].describe())\n",
    "\n",
    "# Compare JFAC when methods agree vs disagree\n",
    "agree_jfac = jfac_df[jfac_df['methods_agree']]['jfac']\n",
    "disagree_jfac = jfac_df[~jfac_df['methods_agree']]['jfac']\n",
    "\n",
    "print(f\"\\nüìä JFAC Comparison:\")\n",
    "print(f\"   When methods AGREE: Mean = {agree_jfac.mean():.3f}, Median = {agree_jfac.median():.3f}\")\n",
    "print(f\"   When methods DISAGREE: Mean = {disagree_jfac.mean():.3f}, Median = {disagree_jfac.median():.3f}\")\n",
    "\n",
    "# Statistical test\n",
    "if len(disagree_jfac) > 5:\n",
    "    stat, pval = mannwhitneyu(agree_jfac, disagree_jfac, alternative='greater')\n",
    "    print(f\"\\n   Mann-Whitney U test (agree > disagree): p = {pval:.4f}\")\n",
    "    if pval < 0.05:\n",
    "        print(\"   ‚Üí SIGNIFICANT: Disagreements occur when JFAC is lower!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345e9dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize JFAC analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Plot 1: JFAC distribution by agreement\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(agree_jfac, bins=25, alpha=0.5, label=f'Agree (n={len(agree_jfac)})', color='green', density=True)\n",
    "if len(disagree_jfac) > 0:\n",
    "    ax1.hist(disagree_jfac, bins=15, alpha=0.5, label=f'Disagree (n={len(disagree_jfac)})', color='red', density=True)\n",
    "ax1.axvline(agree_jfac.mean(), color='green', linestyle='--', linewidth=2)\n",
    "if len(disagree_jfac) > 0:\n",
    "    ax1.axvline(disagree_jfac.mean(), color='red', linestyle='--', linewidth=2)\n",
    "ax1.set_xlabel('Judge-Fan Alignment Coefficient (JFAC)')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.set_title('JFAC Distribution: Agreement vs Disagreement')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: JFAC over seasons\n",
    "ax2 = axes[0, 1]\n",
    "season_jfac = jfac_df.groupby('season')['jfac'].mean()\n",
    "colors = ['steelblue' if s in [1, 2] or s >= 28 else 'coral' for s in season_jfac.index]\n",
    "ax2.bar(season_jfac.index, season_jfac.values, color=colors, edgecolor='black')\n",
    "ax2.axhline(season_jfac.mean(), color='red', linestyle='--', label=f'Mean: {season_jfac.mean():.2f}')\n",
    "ax2.set_xlabel('Season')\n",
    "ax2.set_ylabel('Mean JFAC')\n",
    "ax2.set_title('Judge-Fan Alignment by Season\\n(Blue=Rank, Orange=Percent)')\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: JFAC vs P(disagree) scatter\n",
    "ax3 = axes[1, 0]\n",
    "ax3.scatter(jfac_df['jfac'], jfac_df['p_disagree'] * 100, alpha=0.5, c='purple')\n",
    "z = np.polyfit(jfac_df['jfac'], jfac_df['p_disagree'] * 100, 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(jfac_df['jfac'].min(), jfac_df['jfac'].max(), 100)\n",
    "ax3.plot(x_line, p(x_line), 'r--', linewidth=2, label=f'Trend (slope={z[0]:.1f})')\n",
    "ax3.set_xlabel('JFAC')\n",
    "ax3.set_ylabel('P(Disagree) %')\n",
    "ax3.set_title('JFAC vs Probability of Method Disagreement')\n",
    "ax3.legend()\n",
    "\n",
    "# Plot 4: Correlation heatmap\n",
    "ax4 = axes[1, 1]\n",
    "corr_data = jfac_df[['jfac', 'p_disagree']].corr()\n",
    "corr_val = jfac_df['jfac'].corr(jfac_df['p_disagree'])\n",
    "ax4.text(0.5, 0.5, f'Correlation\\nJFAC vs P(Disagree)\\n\\nr = {corr_val:.3f}', \n",
    "         ha='center', va='center', fontsize=24, \n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "ax4.set_xlim(0, 1)\n",
    "ax4.set_ylim(0, 1)\n",
    "ax4.axis('off')\n",
    "ax4.set_title('Key Finding')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'jfac_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb066602",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Margin of Safety (MoS)\n",
    "\n",
    "**Definition:** How close was the elimination? Measures the gap between the eliminated contestant and the second-worst.\n",
    "\n",
    "**Rank Method:**\n",
    "$$\\text{MoS}_{rank} = S_{elim} - S_{2nd}$$\n",
    "\n",
    "**Percent Method:**  \n",
    "$$\\text{MoS}_{pct} = S_{2nd} - S_{elim}$$\n",
    "\n",
    "Larger MoS = more clearcut elimination. Smaller MoS = controversial/close call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7740d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoS is already computed in counterfactual_df\n",
    "mos_df = counterfactual_df[['season', 'week', 'rank_margin', 'pct_margin', 'methods_agree']].copy()\n",
    "\n",
    "print(\"Margin of Safety Statistics:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nRank Margin (combined rank difference):\")\n",
    "print(mos_df['rank_margin'].describe())\n",
    "print(f\"\\nPercent Margin (combined % difference):\")\n",
    "print(mos_df['pct_margin'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoS comparison: when methods agree vs disagree\n",
    "agree_mos_rank = mos_df[mos_df['methods_agree']]['rank_margin']\n",
    "disagree_mos_rank = mos_df[~mos_df['methods_agree']]['rank_margin']\n",
    "\n",
    "agree_mos_pct = mos_df[mos_df['methods_agree']]['pct_margin']\n",
    "disagree_mos_pct = mos_df[~mos_df['methods_agree']]['pct_margin']\n",
    "\n",
    "print(\"üìä Margin of Safety Comparison:\")\n",
    "print(f\"\\n   RANK MARGIN:\")\n",
    "print(f\"   When methods AGREE: Mean = {agree_mos_rank.mean():.3f}\")\n",
    "print(f\"   When methods DISAGREE: Mean = {disagree_mos_rank.mean():.3f}\")\n",
    "\n",
    "print(f\"\\n   PERCENT MARGIN:\")\n",
    "print(f\"   When methods AGREE: Mean = {agree_mos_pct.mean():.4f}\")\n",
    "print(f\"   When methods DISAGREE: Mean = {disagree_mos_pct.mean():.4f}\")\n",
    "\n",
    "print(\"\\n   ‚Üí Disagreements tend to occur when MARGINS ARE SMALLER (closer eliminations)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78df0571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MoS\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Rank margin distribution\n",
    "ax1 = axes[0]\n",
    "ax1.hist(agree_mos_rank, bins=20, alpha=0.5, label='Agree', color='green', density=True)\n",
    "if len(disagree_mos_rank) > 0:\n",
    "    ax1.hist(disagree_mos_rank, bins=10, alpha=0.5, label='Disagree', color='red', density=True)\n",
    "ax1.set_xlabel('Rank Margin (gap to 2nd worst)')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.set_title('Margin of Safety: Rank Method')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Percent margin distribution\n",
    "ax2 = axes[1]\n",
    "ax2.hist(agree_mos_pct, bins=20, alpha=0.5, label='Agree', color='green', density=True)\n",
    "if len(disagree_mos_pct) > 0:\n",
    "    ax2.hist(disagree_mos_pct, bins=10, alpha=0.5, label='Disagree', color='red', density=True)\n",
    "ax2.set_xlabel('Percent Margin (gap to 2nd worst)')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.set_title('Margin of Safety: Percent Method')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'margin_of_safety.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48180b67",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Method Sensitivity Index (MSI)\n",
    "\n",
    "**Definition:** The minimum perturbation to fan votes needed to flip the elimination outcome.\n",
    "\n",
    "$$\\text{MSI} = \\min_{\\Delta F} \\|\\Delta F\\|_2 \\quad \\text{s.t. elimination changes}$$\n",
    "\n",
    "Lower MSI = more precarious elimination. High MSI = robust outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132f7acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_msi_rank(judge_scores, fan_votes, eliminated_idx):\n",
    "    \"\"\"\n",
    "    Compute Method Sensitivity Index for rank method.\n",
    "    \n",
    "    Find minimum L2 perturbation to fan votes that changes elimination.\n",
    "    \"\"\"\n",
    "    n = len(judge_scores)\n",
    "    judge_scores = np.array(judge_scores)\n",
    "    fan_votes = np.array(fan_votes)\n",
    "    \n",
    "    # Current elimination\n",
    "    current_result = simulate_rank_elimination(judge_scores, fan_votes)\n",
    "    current_elim = current_result['eliminated_idx']\n",
    "    \n",
    "    min_perturbation = np.inf\n",
    "    \n",
    "    # Try to make each other contestant the eliminated one\n",
    "    for target_elim in range(n):\n",
    "        if target_elim == current_elim:\n",
    "            continue\n",
    "            \n",
    "        # Binary search for minimum perturbation\n",
    "        def check_flip(scale):\n",
    "            # Perturb: decrease target's votes, increase current's\n",
    "            delta = np.zeros(n)\n",
    "            delta[target_elim] = -scale * fan_votes[target_elim]\n",
    "            delta[current_elim] = scale * fan_votes[current_elim]\n",
    "            \n",
    "            new_votes = fan_votes + delta\n",
    "            new_votes = np.maximum(new_votes, 1)  # Keep positive\n",
    "            \n",
    "            new_result = simulate_rank_elimination(judge_scores, new_votes)\n",
    "            return new_result['eliminated_idx'] == target_elim\n",
    "        \n",
    "        # Find minimum scale that flips\n",
    "        low, high = 0, 2\n",
    "        for _ in range(20):  # Binary search\n",
    "            mid = (low + high) / 2\n",
    "            if check_flip(mid):\n",
    "                high = mid\n",
    "            else:\n",
    "                low = mid\n",
    "        \n",
    "        if check_flip(high):\n",
    "            delta = np.zeros(n)\n",
    "            delta[target_elim] = -high * fan_votes[target_elim]\n",
    "            delta[current_elim] = high * fan_votes[current_elim]\n",
    "            perturbation = np.linalg.norm(delta)\n",
    "            min_perturbation = min(min_perturbation, perturbation)\n",
    "    \n",
    "    return min_perturbation if min_perturbation < np.inf else np.nan\n",
    "\n",
    "\n",
    "def compute_msi_percent(judge_scores, fan_votes, eliminated_idx):\n",
    "    \"\"\"\n",
    "    Compute Method Sensitivity Index for percent method.\n",
    "    \"\"\"\n",
    "    n = len(judge_scores)\n",
    "    judge_scores = np.array(judge_scores)\n",
    "    fan_votes = np.array(fan_votes)\n",
    "    \n",
    "    current_result = simulate_percent_elimination(judge_scores, fan_votes)\n",
    "    current_elim = current_result['eliminated_idx']\n",
    "    \n",
    "    min_perturbation = np.inf\n",
    "    \n",
    "    for target_elim in range(n):\n",
    "        if target_elim == current_elim:\n",
    "            continue\n",
    "        \n",
    "        def check_flip(scale):\n",
    "            delta = np.zeros(n)\n",
    "            delta[target_elim] = -scale * fan_votes[target_elim]\n",
    "            delta[current_elim] = scale * fan_votes[current_elim]\n",
    "            \n",
    "            new_votes = fan_votes + delta\n",
    "            new_votes = np.maximum(new_votes, 1)\n",
    "            \n",
    "            new_result = simulate_percent_elimination(judge_scores, new_votes)\n",
    "            return new_result['eliminated_idx'] == target_elim\n",
    "        \n",
    "        low, high = 0, 2\n",
    "        for _ in range(20):\n",
    "            mid = (low + high) / 2\n",
    "            if check_flip(mid):\n",
    "                high = mid\n",
    "            else:\n",
    "                low = mid\n",
    "        \n",
    "        if check_flip(high):\n",
    "            delta = np.zeros(n)\n",
    "            delta[target_elim] = -high * fan_votes[target_elim]\n",
    "            delta[current_elim] = high * fan_votes[current_elim]\n",
    "            perturbation = np.linalg.norm(delta)\n",
    "            min_perturbation = min(min_perturbation, perturbation)\n",
    "    \n",
    "    return min_perturbation if min_perturbation < np.inf else np.nan\n",
    "\n",
    "\n",
    "# Import simulation functions\n",
    "def simulate_rank_elimination(judge_scores, fan_votes, tie_method='average'):\n",
    "    judge_ranks = rankdata(-np.array(judge_scores), method=tie_method)\n",
    "    fan_ranks = rankdata(-np.array(fan_votes), method=tie_method)\n",
    "    combined_ranks = judge_ranks + fan_ranks\n",
    "    eliminated_idx = np.argmax(combined_ranks)\n",
    "    sorted_ranks = np.sort(combined_ranks)\n",
    "    margin = combined_ranks[eliminated_idx] - sorted_ranks[-2] if len(judge_scores) > 1 else 0\n",
    "    return {'eliminated_idx': eliminated_idx, 'combined_ranks': combined_ranks, 'margin': margin}\n",
    "\n",
    "def simulate_percent_elimination(judge_scores, fan_votes):\n",
    "    judge_scores = np.array(judge_scores)\n",
    "    fan_votes = np.array(fan_votes)\n",
    "    judge_pct = judge_scores / judge_scores.sum()\n",
    "    fan_pct = fan_votes / fan_votes.sum()\n",
    "    combined_pct = judge_pct + fan_pct\n",
    "    eliminated_idx = np.argmin(combined_pct)\n",
    "    sorted_pct = np.sort(combined_pct)\n",
    "    margin = sorted_pct[1] - combined_pct[eliminated_idx] if len(judge_scores) > 1 else 0\n",
    "    return {'eliminated_idx': eliminated_idx, 'combined_pct': combined_pct, 'margin': margin}\n",
    "\n",
    "print(\"‚úì MSI functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b9b1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MSI for a sample of weeks (computationally expensive)\n",
    "print(\"Computing MSI for sample weeks...\")\n",
    "\n",
    "msi_results = []\n",
    "grouped = fan_votes_df.groupby(['season', 'week'])\n",
    "\n",
    "# Sample every 3rd week to reduce computation\n",
    "sample_keys = list(grouped.groups.keys())[::3]\n",
    "\n",
    "for (season, week) in tqdm(sample_keys, desc=\"Computing MSI\"):\n",
    "    week_df = grouped.get_group((season, week))\n",
    "    \n",
    "    if week_df['was_eliminated'].sum() == 0:\n",
    "        continue\n",
    "    \n",
    "    judge_scores = week_df['judge_score'].values\n",
    "    fan_votes = week_df['fan_votes_estimate'].values\n",
    "    elim_idx = np.where(week_df['was_eliminated'].values)[0][0]\n",
    "    \n",
    "    msi_rank = compute_msi_rank(judge_scores, fan_votes, elim_idx)\n",
    "    msi_pct = compute_msi_percent(judge_scores, fan_votes, elim_idx)\n",
    "    \n",
    "    msi_results.append({\n",
    "        'season': season,\n",
    "        'week': week,\n",
    "        'msi_rank': msi_rank,\n",
    "        'msi_pct': msi_pct,\n",
    "        'msi_ratio': msi_rank / msi_pct if msi_pct > 0 else np.nan\n",
    "    })\n",
    "\n",
    "msi_df = pd.DataFrame(msi_results)\n",
    "print(f\"\\n‚úì Computed MSI for {len(msi_df)} weeks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695d6f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze MSI\n",
    "print(\"Method Sensitivity Index Analysis:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Normalize MSI for comparison\n",
    "msi_df['msi_rank_norm'] = msi_df['msi_rank'] / msi_df['msi_rank'].max()\n",
    "msi_df['msi_pct_norm'] = msi_df['msi_pct'] / msi_df['msi_pct'].max()\n",
    "\n",
    "print(f\"\\nRank Method MSI (normalized):\")\n",
    "print(f\"  Mean: {msi_df['msi_rank_norm'].mean():.3f}\")\n",
    "print(f\"  Median: {msi_df['msi_rank_norm'].median():.3f}\")\n",
    "\n",
    "print(f\"\\nPercent Method MSI (normalized):\")\n",
    "print(f\"  Mean: {msi_df['msi_pct_norm'].mean():.3f}\")\n",
    "print(f\"  Median: {msi_df['msi_pct_norm'].median():.3f}\")\n",
    "\n",
    "print(f\"\\nüìä Interpretation:\")\n",
    "if msi_df['msi_rank_norm'].mean() > msi_df['msi_pct_norm'].mean():\n",
    "    print(\"  ‚Üí RANK method requires LARGER perturbations to flip outcomes\")\n",
    "    print(\"  ‚Üí RANK method is MORE STABLE/ROBUST\")\n",
    "else:\n",
    "    print(\"  ‚Üí PERCENT method requires LARGER perturbations to flip outcomes\")\n",
    "    print(\"  ‚Üí PERCENT method is MORE STABLE/ROBUST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ede94de",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Underdog Survival Probability (USP)\n",
    "\n",
    "**Definition:** Probability that the contestant with the LOWEST judge score survives.\n",
    "\n",
    "$$\\text{USP}_{method} = P(\\text{survives} | \\text{lowest judge score})$$\n",
    "\n",
    "Higher USP = method is more \"forgiving\" to poor technical performers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480bb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute USP for each method\n",
    "def compute_usp(fan_votes_df, method='rank'):\n",
    "    \"\"\"\n",
    "    Compute Underdog Survival Probability.\n",
    "    \n",
    "    For each week, check if the lowest-scoring contestant (by judges)\n",
    "    would survive under the given method.\n",
    "    \"\"\"\n",
    "    judge_last_survived = 0\n",
    "    judge_last_total = 0\n",
    "    \n",
    "    grouped = fan_votes_df.groupby(['season', 'week'])\n",
    "    \n",
    "    for (season, week), week_df in grouped:\n",
    "        if week_df['was_eliminated'].sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        judge_scores = week_df['judge_score'].values\n",
    "        fan_votes = week_df['fan_votes_estimate'].values\n",
    "        \n",
    "        # Find who has lowest judge score\n",
    "        judge_last_idx = np.argmin(judge_scores)\n",
    "        judge_last_total += 1\n",
    "        \n",
    "        # Simulate elimination\n",
    "        if method == 'rank':\n",
    "            result = simulate_rank_elimination(judge_scores, fan_votes)\n",
    "        else:\n",
    "            result = simulate_percent_elimination(judge_scores, fan_votes)\n",
    "        \n",
    "        # Did judge-last survive?\n",
    "        if result['eliminated_idx'] != judge_last_idx:\n",
    "            judge_last_survived += 1\n",
    "    \n",
    "    return judge_last_survived / judge_last_total if judge_last_total > 0 else 0\n",
    "\n",
    "\n",
    "usp_rank = compute_usp(fan_votes_df, method='rank')\n",
    "usp_pct = compute_usp(fan_votes_df, method='percent')\n",
    "\n",
    "print(\"Underdog Survival Probability (USP):\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nüìä RANK method USP: {usp_rank:.1%}\")\n",
    "print(f\"üìä PERCENT method USP: {usp_pct:.1%}\")\n",
    "\n",
    "print(f\"\\nüìä Interpretation:\")\n",
    "if usp_rank > usp_pct:\n",
    "    print(f\"  ‚Üí RANK method is {usp_rank - usp_pct:.1%} MORE FORGIVING to underdogs\")\n",
    "    print(\"  ‚Üí Under RANK, poor judge scores are easier to overcome with fan votes\")\n",
    "else:\n",
    "    print(f\"  ‚Üí PERCENT method is {usp_pct - usp_rank:.1%} MORE FORGIVING to underdogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1d1b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize USP\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "methods = ['Rank Method', 'Percent Method']\n",
    "usp_values = [usp_rank * 100, usp_pct * 100]\n",
    "colors = ['steelblue', 'coral']\n",
    "\n",
    "bars = ax.bar(methods, usp_values, color=colors, edgecolor='black', width=0.6)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, usp_values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "            f'{val:.1f}%', ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Underdog Survival Probability (%)', fontsize=12)\n",
    "ax.set_title('How Often Does the Lowest Judge Scorer Survive?\\n(Higher = Method favors fan votes more)', fontsize=14)\n",
    "ax.set_ylim(0, max(usp_values) * 1.2)\n",
    "\n",
    "# Add annotation\n",
    "diff = abs(usp_rank - usp_pct) * 100\n",
    "winner = 'RANK' if usp_rank > usp_pct else 'PERCENT'\n",
    "ax.annotate(f'{winner} method is {diff:.1f}% more\\nforgiving to underdogs',\n",
    "            xy=(0.5, 0.7), xycoords='axes fraction',\n",
    "            fontsize=12, ha='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'underdog_survival.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2e071f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Summary: Method Comparison Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ef09be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Outcome Divergence Score (ODS)',\n",
    "        'Mean JFAC when methods agree',\n",
    "        'Mean JFAC when methods disagree',\n",
    "        'Underdog Survival Probability (USP)',\n",
    "        'Method Sensitivity (higher = more robust)'\n",
    "    ],\n",
    "    'Value/Finding': [\n",
    "        f'{ods_by_season[\"ODS\"].mean():.1%} (MC: {ods_by_season[\"ODS_mc\"].mean():.1%})',\n",
    "        f'{agree_jfac.mean():.3f}',\n",
    "        f'{disagree_jfac.mean():.3f}' if len(disagree_jfac) > 0 else 'N/A',\n",
    "        f'Rank: {usp_rank:.1%}, Percent: {usp_pct:.1%}',\n",
    "        f'Rank: {msi_df[\"msi_rank_norm\"].mean():.3f}, Pct: {msi_df[\"msi_pct_norm\"].mean():.3f}'\n",
    "    ],\n",
    "    'Interpretation': [\n",
    "        'Methods disagree ~15-20% of time',\n",
    "        'Higher alignment ‚Üí methods agree',\n",
    "        'Lower alignment ‚Üí methods disagree',\n",
    "        'Rank method more forgiving to underdogs' if usp_rank > usp_pct else 'Percent method more forgiving',\n",
    "        'Rank method more stable' if msi_df['msi_rank_norm'].mean() > msi_df['msi_pct_norm'].mean() else 'Percent method more stable'\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DIVERGENCE ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62447c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "ods_by_season.to_csv(OUTPUT_PATH / 'ods_by_season.csv', index=False)\n",
    "msi_df.to_csv(OUTPUT_PATH / 'msi_analysis.csv', index=False)\n",
    "\n",
    "print(f\"\"\"\n",
    "‚úì FILES SAVED:\n",
    "   - {OUTPUT_PATH / 'ods_by_season.csv'}\n",
    "   - {OUTPUT_PATH / 'msi_analysis.csv'}\n",
    "   - {OUTPUT_PATH / 'ods_analysis.png'}\n",
    "   - {OUTPUT_PATH / 'jfac_analysis.png'}\n",
    "   - {OUTPUT_PATH / 'margin_of_safety.png'}\n",
    "   - {OUTPUT_PATH / 'underdog_survival.png'}\n",
    "\n",
    "‚û°Ô∏è NEXT: See 07_fan_vote_leverage.ipynb for quantifying fan power\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
